{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "from keras import Model\n",
    "\n",
    "import pickle    \n",
    "import os\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "                \n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\n",
    "submission = pd.read_csv('/kaggle/input/google-quest-challenge/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"/kaggle/input/universalsentenceencoderlarge4/\"\n",
    "embed = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the keras Lambda\n",
    "def UniversalEmbedding(x):\n",
    "    results = embed(tf.squeeze(tf.cast(x, tf.string)))[\"outputs\"]\n",
    "    print(results)\n",
    "    return keras.backend.concatenate([results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup training data\n",
    "targets = [\n",
    "        'question_asker_intent_understanding',\n",
    "        'question_body_critical',\n",
    "        'question_conversational',\n",
    "        'question_expect_short_answer',\n",
    "        'question_fact_seeking',\n",
    "        'question_has_commonly_accepted_answer',\n",
    "        'question_interestingness_others',\n",
    "        'question_interestingness_self',\n",
    "        'question_multi_intent',\n",
    "        'question_not_really_a_question',\n",
    "        'question_opinion_seeking',\n",
    "        'question_type_choice',\n",
    "        'question_type_compare',\n",
    "        'question_type_consequence',\n",
    "        'question_type_definition',\n",
    "        'question_type_entity',\n",
    "        'question_type_instructions',\n",
    "        'question_type_procedure',\n",
    "        'question_type_reason_explanation',\n",
    "        'question_type_spelling',\n",
    "        'question_well_written',\n",
    "        'answer_helpful',\n",
    "        'answer_level_of_information',\n",
    "        'answer_plausible',\n",
    "        'answer_relevance',\n",
    "        'answer_satisfaction',\n",
    "        'answer_type_instructions',\n",
    "        'answer_type_procedure',\n",
    "        'answer_type_reason_explanation',\n",
    "        'answer_well_written'    \n",
    "    ]\n",
    "\n",
    "input_columns = ['question_title','question_body','answer']\n",
    "\n",
    "X1 = train[input_columns[0]].values.tolist()\n",
    "X2 = train[input_columns[1]].values.tolist()\n",
    "X3 = train[input_columns[2]].values.tolist()\n",
    "X1 = [x.replace('?','.').replace('!','.') for x in X1]\n",
    "X2 = [x.replace('?','.').replace('!','.') for x in X2]\n",
    "X3 = [x.replace('?','.').replace('!','.') for x in X3]\n",
    "\n",
    "X = [X1,X2,X3]\n",
    "y = train[targets].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build network\n",
    "def swish(x):\n",
    "    return K.sigmoid(x) * x\n",
    "\n",
    "embed_size = 512 #must be 512 for univerasl embedding layer\n",
    "\n",
    "input_text1 = Input(shape=(1,), dtype=tf.string)\n",
    "embedding1 = Lambda(UniversalEmbedding, output_shape=(embed_size,))(input_text1)\n",
    "input_text2 = Input(shape=(1,), dtype=tf.string)\n",
    "embedding2 = Lambda(UniversalEmbedding, output_shape=(embed_size,))(input_text2)\n",
    "input_text3 = Input(shape=(1,), dtype=tf.string)\n",
    "embedding3 = Lambda(UniversalEmbedding, output_shape=(embed_size,))(input_text3)\n",
    "\n",
    "x = Concatenate()([embedding1,embedding2,embedding3])\n",
    "x = Dense(256, activation=swish)(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(64, activation=swish, kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = BatchNormalization()(x)\n",
    "output = Dense(len(targets),activation='sigmoid',name='output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_text1,input_text2,input_text3], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up as much as possible\n",
    "import gc\n",
    "print(gc.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=2, min_lr=1e-7, verbose=1)\n",
    "optimizer = Adadelta()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "model.fit(X, [y], epochs=20, validation_split=.1,batch_size=32,callbacks=[reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep test data\n",
    "X1 = test[input_columns[0]].values.tolist()\n",
    "X2 = test[input_columns[1]].values.tolist()\n",
    "X3 = test[input_columns[2]].values.tolist()\n",
    "X1 = [x.replace('?','.').replace('!','.') for x in X1]\n",
    "X2 = [x.replace('?','.').replace('!','.') for x in X2]\n",
    "X3 = [x.replace('?','.').replace('!','.') for x in X3]\n",
    "\n",
    "pred_X = [X1,X2,X3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "pred_y = model.predict(pred_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the submission\n",
    "submission = pd.read_csv('/kaggle/input/google-quest-challenge/sample_submission.csv')\n",
    "submission[targets] = pred_y\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result\n",
    "submission.to_csv(\"submission.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
